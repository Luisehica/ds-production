{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x7f017418acf8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    ")\n",
    " \n",
    "data = {'hello': 'world', 'time': time.time()}\n",
    "producer.send('dsp', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'dsp',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    " \n",
    "for x in consumer:\n",
    "    print(x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "external_ip = os.getenv('kafka.bootstrap.servers')\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{external_ip}:9092\")\n",
    "    .option(\"subscribe\", \"dsp\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .withColumn(\"value_deserialized\", \n",
    "                udf(lambda x: json.loads(x.decode('utf-8')))(\"value\"))\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecordMetadata(topic='dsp', partition=0, topic_partition=TopicPartition(topic='dsp', partition=0), offset=109, timestamp=1576709681368, checksum=None, serialized_key_size=-1, serialized_value_size=142, serialized_header_size=-1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from kafka import KafkaProducer\n",
    "from json import dumps\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=['54.166.148.190:9092'],\n",
    "          value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "data = { 'G1': 1, 'G2': 0, 'G3': 0, 'G4': 0, 'G5': 0, \n",
    "         'G6': 0, 'G7': 0, 'G8': 0, 'G9': 0, 'G10': 0, \n",
    "        'User_ID': str(uuid.uuid1())}\n",
    "result = producer.send('dsp', data)\n",
    "result.get()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StringType\n",
    "import json \n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# build a logsitic regression model \n",
    "gamesDF = pd.read_csv(\"https://github.com/bgweber/Twitch/raw/master/Recommendations/games-expand.csv\")\n",
    "model = LogisticRegression() \n",
    "model.fit(gamesDF.iloc[:,0:10], gamesDF['label'])\n",
    "\n",
    "# read from Kafka \n",
    "df = spark .readStream.format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"54.166.148.190:9092\") \\\n",
    "  .option(\"subscribe\", \"dsp\").load()\n",
    "\n",
    "# define the UDF for scoring users \n",
    "def score(row):\n",
    "    d = json.loads(row)\n",
    "    p = pd.DataFrame.from_dict(d, orient = \"index\").transpose()        \n",
    "    pred = model.predict_proba(p.iloc[:,0:10])[0][0]\n",
    "    result = {'User_ID': d['User_ID'], 'pred': pred }\n",
    "    return str(json.dumps(result))\n",
    "    \n",
    "# select the value field and apply the UDF     \n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "score_udf = udf(score, StringType())    \n",
    "df = df.select( score_udf(\"value\").alias(\"value\"))\n",
    "\n",
    "# Write results to Kafka \n",
    "query = df.writeStream.format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"54.166.148.190:9092\") \\\n",
    "  .option(\"topic\", \"preds\") \\\n",
    "  .option(\"checkpointLocation\", \"/temp\").start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "\n",
    "consumer = KafkaConsumer('preds',\n",
    "     bootstrap_servers=['54.166.148.190:9092'],\n",
    "     value_deserializer=lambda x: loads(x))\n",
    "\n",
    "for x in consumer:\n",
    "    print(x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataflow Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PubSub Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "subscription_path = subscriber.subscription_path(\"gameanalytics-199018\", \"dsp\")\n",
    "\n",
    "def callback(message):\n",
    "    print(message.data)\n",
    "    message.ack()\n",
    "\n",
    "subscriber.subscribe(subscription_path, callback=callback)\n",
    "\n",
    "while True:\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PubSub Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import pubsub_v1\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "topic_path = publisher.topic_path(\"gameanalytics-199018\", \"natality\")\n",
    "\n",
    "data = \"Hello World!\".encode('utf-8')\n",
    "publisher.publish(topic_path, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import argparse\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.io.gcp.bigquery import parse_table_schema_from_json\n",
    "import json\n",
    "\n",
    "class ApplyDoFn(beam.DoFn):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._model = None\n",
    "        from google.cloud import storage\n",
    "        import pandas as pd\n",
    "        import pickle as pkl\n",
    "        import json as js\n",
    "        self._storage = storage\n",
    "        self._pkl = pkl\n",
    "        self._pd = pd\n",
    "        self._json = js\n",
    "     \n",
    "    def process(self, element):\n",
    "        if self._model is None:\n",
    "            bucket = self._storage.Client().get_bucket(\n",
    "                                                 'dsp_model_store')\n",
    "            blob = bucket.get_blob('natality/sklearn-linear')\n",
    "            self._model =self._pkl.loads(blob.download_as_string())\n",
    "        \n",
    "        element = self._json.loads(element.decode('utf-8'))\n",
    "        new_x = self._pd.DataFrame.from_dict(element, \n",
    "                            orient = \"index\").transpose().fillna(0)   \n",
    "        weight = self._model.predict(new_x.iloc[:,1:8])[0]\n",
    "        return [ { 'guid': element['guid'], 'weight': weight, \n",
    "                                   'time': str(element['time']) } ]\n",
    "             \n",
    "class PublishDoFn(beam.DoFn):\n",
    "    \n",
    "    def __init__(self):\n",
    "        from google.cloud import datastore       \n",
    "        self._ds = datastore\n",
    "    \n",
    "    def process(self, element):\n",
    "        client = self._ds.Client()\n",
    "        key = client.key('natality-guid', element['guid'])\n",
    "        entity = self._ds.Entity(key)\n",
    "        entity['weight'] = element['weight']         \n",
    "        entity['time'] = element['time']\n",
    "        client.put(entity)\n",
    "            \n",
    "# set up pipeline parameters \n",
    "parser = argparse.ArgumentParser()\n",
    "known_args, pipeline_args = parser.parse_known_args(None)\n",
    "pipeline_options = PipelineOptions(pipeline_args)\n",
    "\n",
    "# define the topics \n",
    "topic = \"projects/{project}/topics/{topic}\"\n",
    "topic = topic.format(project = \"gameanalytics-199018\", topic = \"natality\")\n",
    "\n",
    "schema = parse_table_schema_from_json(json.dumps({'fields':\n",
    "            [ { 'name': 'guid', 'type': 'STRING'},\n",
    "              { 'name': 'weight', 'type': 'FLOAT64'},\n",
    "              { 'name': 'time', 'type': 'STRING'} ]}))\n",
    "\n",
    "# define the pipeline steps \n",
    "p = beam.Pipeline(options=pipeline_options)\n",
    "lines = p | 'Read PubSub' >> beam.io.ReadFromPubSub(topic=topic)\n",
    "scored = lines | 'apply' >> beam.ParDo(ApplyDoFn())\n",
    "scored | 'Create entities' >> beam.ParDo(PublishDoFn())\n",
    "\n",
    "# run the pipeline \n",
    "result = p.run()\n",
    "result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.pubsub_v1.publisher.futures.Future at 0x7f0f866bebd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from google.cloud import pubsub_v1\n",
    "import time \n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "topic_path = publisher.topic_path(\"gameanalytics-199018\", \"natality\")\n",
    "\n",
    "data = json.dumps({'year': 2001, 'plurality': 1, 'apgar_5min': 99, 'mother_age': 33, \n",
    "     'father_age': 40, 'gestation_weeks': 38, 'ever_born': 8, \n",
    "     'mother_married': 1, 'weight': 6.8122838958, \n",
    "     'time': str(time.time()), \n",
    "     'guid': 'b281c5e8-85b2-4cbd-a2d8-e501ca816363'}\n",
    ").encode('utf-8') \n",
    "\n",
    "publisher.publish(topic_path, data=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
